# TI3150TU Capstone Applied AI project



## Getting started
Introduction:

This repository is for the Capstone Applied AI Project, where the goal is to create an invertible neural network (INN) based on a coupling flow architecture to simulate a preconditioner for solving large linear systems. It learns a preconditioning map based on a synthetically generated Poisson linear system.

In many large linear systems, solving them becomes less of an issue of complexity, but more of a memory issue. This project aims to reduce the amount of iterations required to reach convergance on an iterative solver such as the Richardson method. 

## Prerequisites

    -> Python 3.10+

    -> PyTorch 2.x

    -> NumPy

    -> SciPy

It is strongly recommended to use a virtual environment as some older versions of packages could be installed, causing issues.


## Repository Structure

```text
.
├── data/
│   └── data_generation.py        # Poisson matrix + vector generation
├── applications/
│   └── applicationAndTest.py
├── models/
│   ├── __init__.py
│   ├── model_supervised.py       # Supervised INN and MLP training
│   └── model_unsupervised.py
├── utils/
│   └── __init__.py
└── README.md
```



data_generation.py must not be modified

All learning is performed explicitly and manually in model.py

## Problem Setup

We consider a linear system induced by a Poisson discretization:

    -> v=Aw

where:
A is a fixed sparse Poisson matrix generated by poisson_gene,
w is a random input vector,
v is the resulting residual / observation

## Data Generation

Training data is generated using the following files:
```text
from data.data_generation import poisson_gene, gen_vec
```
poisson_gene(nx, ny) constructs the Poisson operator

gen_vec(dim, samples, A) generates paired samples (v, w) such that:
<p align="center">
  v = A @ w
</p>

<p align="center">
  w ~ N(0, I)
</p>

## Models
Supervised INN:

    -> Built from manual affine coupling layers

    -> Alternating split structure (with flipping)

    -> Identity initialization for stability

    -> Trained using MSE loss

    -> Double precision (float64) throughout

    -> This model preserves invertibility structure while being trained purely in a supervised fashion.

Unsupervised INN:

    -> Built from manual affine coupling layers

    -> Alternating split structure (with flipping)

    -> Identity initialization for stability

    -> Trained using a physics-based MSE loss enforcing: Aw=v

    -> No access to ground-truth solutions during training

    -> Double precision (float64) throughout


Baseline MLP

    -> A standard fully-connected network with the same input/output dimensionality, used for comparison, nothing really special. 


## Training
To train any of the models, simply run their respective files. for isntance, running model_supervised.py returns a supervisedly trained INN.


## Usage

After training, the model can be used and tested in the applicationAndTest.py which times the selected model, and compares the number of iterations required to reach converagnce with traditonal 'mathematical' solvers, such as Gauss and Jacobi. 


## Support
If an issue arrises please have a CHAT with a Generative Pretrained Transformer.

## Contributing
Please do not contact us about contribution, please just edit the files yourself.

## Authors and acknowledgment
This is capstone group 3, many thanks to all people who contributed as well as our TAs and our supervisors.

## License
This project is open source, please feel free to use and copy this code as much as you want. 


