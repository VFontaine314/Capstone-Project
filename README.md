# TI3150TU Capstone Applied AI project



## Getting started
Introduction:

This repository is for the Capstone Applied AI Project, where the goal is to create an invertible neural network (INN) based on a coupling flow architecture to simulate a preconditioner for solving large linear systems. It learns a preconditioning map based on a synthetically generated Poisson linear system.

In many large linear systems, solving them becomes less of an issue of complexity, but more of a memory issue. This project aims to reduce the amount of iterations required to reach convergance on an iterative solver such as the Richardson method. 

## Prerequisites

    -> Python 3.10+

    -> PyTorch 2.x

    -> NumPy

    -> SciPy

It is strongly recommended to use a virtual environment as some older versions of packages could be installed, causing issues.
You may also pip install all using the requirements.txt file.

## Requirements 
The Requirements are divided into 'MoSCoW' sections, which stands for what the project Must, Should, Could, and Would.

Must 1: Must understand the role of a preconditioner in solving linear systems. 

    -> This is a bit hard to prove, but we hope the presentation and code speaks for itself. You may also consult the TA and the supervisors.


Must 2: Must implement INNs to find a preconditioner.

    -> This has been completed. Please refer to applicationAndTest.py which shows our work.


Should 1: Should evaluate the performance of the proposed method on various PDE methods. 

    -> This one has been completed. Please refer to applicationAndTest.py.


Should 2: Should compare the convergence of the proposed method with classical preconditioners. 

    -> This is done, below is a chart for you to see the convergance rates.
    -> The residual is the difference: Ax - b
<img width="967" height="724" alt="image" src="https://github.com/user-attachments/assets/6b1eff6b-0324-47b9-b204-ca8147318493" />


Should 3: Should develop a well-structured Git repository with a focus on clean, reusable, and modular code implementation.

    -> I hope you enjoy reading this repo, we have spend a considerable amount of time learning about git and writing about it.
    -> We belive the repo is clean and easily usable, with clear instructions.


Could 1: Could explore alternative AI models, iterative solvers, and additional application scenarios

    -> Other AI models has been done. Please refer to the MLP.py file, which simply uses an MLP not an INN.
    -> Other iterative solvers has not been done, as the Richardson method is a good baseline
    -> other application scenarios have not been done. 


Could 2: Could test the performance on the DelftBlue supercomputer

    -> This has not been done due to time constraints and the winterbreak wreaking havoc on everyones schedule. 



W1 & Won't have to implement numerical discretization methods for setting up the linear equation systems (this is provided by the supervisors in the data_generation.py)



## Repository Structure

```text
.
├── data/
│   └── data_generation.py        # Poisson matrix + vector generation
├── applications/
│   └── applicationAndTest.py
├── models/
│   ├── __init__.py
│   ├── model_supervised.py       # Supervised INN and MLP training
│   └── model_unsupervised.py
│   └── mlp.py
├── utils/
│   └── __init__.py
└── README.md
└── requirements.txt
```



data_generation.py must not be modified

All learning is performed explicitly and manually in model.py

## Problem Setup

We consider a linear system induced by a Poisson discretization:
<p align="center">
  v = Aw
</p>


where:
A is a fixed sparse Poisson matrix generated by poisson_gene,
w is a random input vector,
v is the resulting residual / observation

## Data Generation

Training data is generated using the following files:
```text
from data.data_generation import poisson_gene, gen_vec
```
poisson_gene(nx, ny) constructs the Poisson operator

gen_vec(dim, samples, A) generates paired samples (v, w) such that:
<p align="center">
  v = A @ w
</p>

<p align="center">
  w ~ N(0, I)
</p>

## Models
Supervised INN:

    -> Built from manual affine coupling layers

    -> Alternating split structure (with flipping)

    -> Identity initialization for stability

    -> Trained using MSE loss

    -> Double precision (float64) throughout

    -> This model preserves invertibility structure while being trained purely in a supervised fashion.

Unsupervised INN:

    -> Built from manual affine coupling layers

    -> Alternating split structure (with flipping)

    -> Identity initialization for stability

    -> Trained using a physics-based MSE loss enforcing: Aw=v

    -> No access to ground-truth solutions during training

    -> Double precision (float64) throughout


Baseline MLP

    -> A standard fully-connected network with the same input/output dimensionality, used for comparison, nothing really special. 


## Training
To train any of the models, simply run their respective files. for isntance, running model_supervised.py returns a supervisedly trained INN.


## Usage

After training, the model can be used and tested in the applicationAndTest.py which times the selected model, and compares the number of iterations required to reach converagnce with traditonal 'mathematical' solvers, such as Gauss and Jacobi. 


## Support
If an issue arrises please have a CHAT with a Generative Pretrained Transformer.

## Contributing
Please do not contact us about contribution, please just edit the files yourself.

## Authors and acknowledgment
This is capstone group 3, many thanks to all people who contributed as well as our TAs and our supervisors.

## License
This project is open source, please feel free to use and copy this code as much as you want. 


