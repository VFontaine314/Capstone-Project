# TI3150TU Capstone Applied AI project



## Getting started
Introduction:

This repository is for the Capstone Applied AI Project, where the goal is to create an invertible neural network (INN) based on a coupling flow architecture to simulate a preconditioner for solving large linear systems. It learns a preconditioning map based on a synthetically generated Poisson linear system.

In many large linear systems, solving them becomes less of an issue of complexity, but more of a memory issue. This project aims to reduce the amount of iterations required to reach convergance on an iterative solver such as the Richardson method. 


1. Prerequisites:

    -> Python 3.10+

    -> PyTorch 2.x

    -> NumPy

    -> SciPy

It is strongly recommended to use a virtual environment as some older versions of packages could be installed, causing issues.


2. Repository Structure

## Repository Structure

```text
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_generation.py        # Poisson matrix + vector generation
â”œâ”€â”€ applications/
â”‚   â””â”€â”€ applicationAndTest.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_supervised.py       # Supervised INN and MLP training
â”‚   â””â”€â”€ model_unsupervised.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ README.md
```



data_generation.py must not be modified

All learning is performed explicitly and manually in model.py

3. Problem Setup

We consider a linear system induced by a Poisson discretization:

ğ‘£
=
ğ´
ğ‘¤
v=Aw

where:

ğ´
A is a fixed sparse Poisson matrix generated by poisson_gene

ğ‘¤
w is a random input vector

ğ‘£
v is the resulting residual / observation

The learning task is supervised regression:

model
(
ğ‘£
)
â‰ˆ
ğ‘¤
model(v)â‰ˆw

This is intentionally not density estimation and does not use likelihood-based losses.

4. Data Generation

Training data is generated using:

from data.data_generation import poisson_gene, gen_vec


poisson_gene(nx, ny) constructs the Poisson operator

gen_vec(dim, samples, A) generates paired samples (v, w) such that:

v = A @ w

w ~ N(0, I)

Inputs (v) are normalized, outputs (w) are not.

5. Models
Supervised INN (Default)

Built from manual affine coupling layers

Alternating split structure (with flipping)

Identity initialization for stability

Trained using MSE loss

Double precision (float64) throughout

This model preserves invertibility structure while being trained purely in a supervised fashion.

Baseline MLP

A standard fully-connected network with the same input/output dimensionality, used for comparison.

6. Training

To train the supervised INN, simply run:

python model/model.py


Key training details:

Loss: Mean Squared Error (MSE)

Optimizer: Adam

Scheduler: ReduceLROnPlateau

Precision: float64 (mandatory)

Batch training with shuffling

Progress is printed every 10 epochs.

7. Evaluation

After training, the script automatically:

Generates a fresh test sample

Applies the same input normalization

Predicts 
ğ‘¤
^
w
^

Reports test MSE

Example output:

Final Test MSE: 1.2e-06

8. Usage

After training, the model can be used as:

w_pred = model((v - v_mean) / v_std)


where v_mean and v_std are the statistics computed during training.



## Support
If an issue arrises please have a CHAT with a Generative Pretrained Transformer.

## Contributing
Please do not contact us about contribution, please just edit the files yourself.

## Authors and acknowledgment
This is capstone group 3, many thanks to all people who contributed as well as our TAs and our supervisors.

## License
This project is open source, please feel free to use and copy this code as much as you want. 


